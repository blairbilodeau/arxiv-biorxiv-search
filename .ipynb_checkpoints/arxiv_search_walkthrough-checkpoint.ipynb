{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import sys\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's specify the parameters as though we were going to pass them to the arxivsearch function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## params\n",
    "start_date = str(datetime.date(2020,5,1))\n",
    "end_date = str(datetime.date(2020,5,15))\n",
    "kwd_req = ['online'] \n",
    "kwd_exc = ['education'] \n",
    "kwd_one = [['bandit', 'partial information'], ['regret', 'generalization error']]\n",
    "athr_req = [] \n",
    "athr_exc = [] \n",
    "athr_one = []\n",
    "subject = 'stat' # subjects = ['stat']\n",
    "max_records = 50\n",
    "max_time = 300\n",
    "cols = ['id', 'title', 'authors', 'date', 'categories', 'abstract']\n",
    "export = '/Users/blairbilodeau/Desktop/arxiv/'\n",
    "exportfile = ''\n",
    "download = '/Users/blairbilodeau/Desktop/arxiv/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try accessing the url, which we print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set url to extract papers from this subject\n",
    "OAI = '{http://www.openarchives.org/OAI/2.0/}'\n",
    "ARXIV = '{http://arxiv.org/OAI/arXiv/}'\n",
    "BASE = 'http://export.arxiv.org/oai2?verb=ListRecords&'\n",
    "\n",
    "url = BASE + 'from=' + start_date + '&until=' + end_date + '&metadataPrefix=arXiv&set=%s' %subject\n",
    "\n",
    "## error handling for opening and closing URL\n",
    "try:\n",
    "\t#url_response = urlopen(url)\n",
    "\t#xml = url_response.read() # get raw xml data from server\n",
    "\tgc.collect()\n",
    "except HTTPError as e:\n",
    "\t# handle service unavailable error for requesting too often\n",
    "\tif e.code == 503:\n",
    "\t\tretry_time = int(e.hdrs.get('retry-after', 30))\n",
    "\t\tprint('Got 503. Retry after {0:d} seconds.'.format(retry_time))\n",
    "\t# if it's a different error, just have to raise it\n",
    "\telse:\n",
    "\t\traise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://export.arxiv.org/oai2?verb=ListRecords&from=2020-05-01&until=2020-05-15&metadataPrefix=arXiv&set=stat\n"
     ]
    }
   ],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a bit how ugly XML is for humans, and start extracting elements from it we can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<OAI-PMH xmlns=\"http://www.openarchives.org/OAI/2.0/\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.openarchives.org/OAI/2'\n"
     ]
    }
   ],
   "source": [
    "print(xml[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Element '{http://www.openarchives.org/OAI/2.0/}OAI-PMH' at 0x11bf27530>\n"
     ]
    }
   ],
   "source": [
    "xml_root = ET.fromstring(xml) # get root of xml hierarchy\n",
    "print(xml_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have fetched 1000 records. Here is a sample.\n",
      "[<Element '{http://www.openarchives.org/OAI/2.0/}record' at 0x11bf27a10>, <Element '{http://www.openarchives.org/OAI/2.0/}record' at 0x11bf51ad0>, <Element '{http://www.openarchives.org/OAI/2.0/}record' at 0x11bf522f0>]\n"
     ]
    }
   ],
   "source": [
    "records = xml_root.findall(OAI + 'ListRecords/' + OAI + 'record') # list of all records from xml tree\n",
    "print('We have fetched {0:d} records. Here is a sample.'.format(len(records)))\n",
    "print(records[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element '{http://arxiv.org/OAI/arXiv/}arXiv' at 0x11bf27ef0>, <Element '{http://arxiv.org/OAI/arXiv/}arXiv' at 0x11bf51d10>, <Element '{http://arxiv.org/OAI/arXiv/}arXiv' at 0x11bf52530>]\n"
     ]
    }
   ],
   "source": [
    "## extract metadata for each record\n",
    "metadata = [record.find(OAI + 'metadata').find(ARXIV + 'arXiv') for record in records]\n",
    "print(metadata[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Python's list comprehension to extract the data. Here's an example of how that works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "y =  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "## list comprehension\n",
    "x = []\n",
    "for i in range(10):\n",
    "\tx = x + [i]\n",
    "print('x = ',x)\n",
    "\n",
    "# v.s.\n",
    "\n",
    "y = [i for i in range(10)]\n",
    "print('y = ',y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract fields from the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use metadata to get info for each record\n",
    "titles = [meta.find(ARXIV + 'title').text.strip().lower().replace('\\n', ' ') for meta in metadata]\n",
    "dates = [meta.find(ARXIV + 'created').text.strip() if meta.find(ARXIV + 'updated') is None else meta.find(ARXIV + 'updated').text.strip() for meta in metadata]\n",
    "ids = [meta.find(ARXIV + 'id').text.strip().lower().replace('\\n', ' ') for meta in metadata]\n",
    "abstracts = [meta.find(ARXIV + 'abstract').text.strip().lower().replace('\\n', ' ') for meta in metadata]\n",
    "category_lists = [meta.find(ARXIV + 'categories').text.strip().lower().replace('\\n', ' ').split() for meta in metadata]\n",
    "urls = ['https://arxiv.org/abs/' + meta.find(ARXIV + 'id').text.strip().lower().replace('\\n', ' ') for meta in metadata]\n",
    "author_lists = [meta.findall(ARXIV + 'authors/' + ARXIV + 'author') for meta in metadata]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate why we use the strip(), lower(), and replace() commands, here's a before and after of the first abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  We prove a Gaussian process approximation for the sequence of random\\ncompositions of a two-color randomly reinforced urn for both the cases with the\\nequal and unequal reinforcement means. By using the Gaussian approximation, the\\nlaw of the iterated logarithm and the functional limit central limit theorem in\\nboth the stable convergence sense and the almost-sure conditional convergence\\nsense are established. Also as a consequence, we are able to to prove that the\\ndistribution of the urn composition has no points masses both when the\\nreinforcement means are equal and unequal under the assumption of only finite\\n$(2+\\\\epsilon)$-th moments.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata[0].find(ARXIV + 'abstract').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we prove a gaussian process approximation for the sequence of random compositions of a two-color randomly reinforced urn for both the cases with the equal and unequal reinforcement means. by using the gaussian approximation, the law of the iterated logarithm and the functional limit central limit theorem in both the stable convergence sense and the almost-sure conditional convergence sense are established. also as a consequence, we are able to to prove that the distribution of the urn composition has no points masses both when the reinforcement means are equal and unequal under the assumption of only finite $(2+\\\\epsilon)$-th moments.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstracts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The author lists currently are still in element form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Element '{http://arxiv.org/OAI/arXiv/}author' at 0x11bf51290>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_lists[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean them up and check it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## extract first and last names, clean them, and put them together to make human readable\n",
    "last_name_lists = [[author.find(ARXIV + 'keyname').text.lower() for author in author_list] for author_list in author_lists]\n",
    "first_name_meta_lists = [[author.find(ARXIV + 'forenames') for author in author_list] for author_list in author_lists]\n",
    "first_name_lists = [['' if name == None else name.text.lower() for name in first_name_meta_list] for first_name_meta_list in first_name_meta_lists]\n",
    "full_name_temp_lists = [zip(a,b) for a,b in zip(first_name_lists, last_name_lists)]\n",
    "full_name_lists = [[a+' '+b for a,b in full_name_temp_list] for full_name_temp_list in full_name_temp_lists]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mossel', 'mueller-frank', 'sly', 'tamuz']\n",
      "['elchanan', 'manuel', 'allan', 'omer']\n",
      "['elchanan mossel', 'manuel mueller-frank', 'allan sly', 'omer tamuz']\n"
     ]
    }
   ],
   "source": [
    "print(last_name_lists[2])\n",
    "print(first_name_lists[2])\n",
    "print(full_name_lists[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to put this into a dataframe (table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                                social learning equilibria\n",
       "date                                                 2019-09-27\n",
       "id                                                    1207.5895\n",
       "abstract      we consider a large class of social learning m...\n",
       "categories                          [math.st, econ.th, stat.th]\n",
       "url                             https://arxiv.org/abs/1207.5895\n",
       "authors       [elchanan mossel, manuel mueller-frank, allan ...\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compile all info into big dataframe\n",
    "records_data = list(zip(titles, dates, ids, abstracts, category_lists, urls, full_name_lists))\n",
    "records_df = pd.DataFrame(records_data,columns=['title','date','id','abstract','categories','url','authors'])\n",
    "records_df.iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can finally start searching to only pick out the papers we desire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "social learning equilibria\n",
      "\n",
      "we consider a large class of social learning models in which a group of agents face uncertainty regarding a state of the world, share the same utility function, observe private signals, and interact in a general dynamic setting. we introduce social learning equilibria, a static equilibrium concept that abstracts away from the details of the given extensive form, but nevertheless captures the corresponding asymptotic equilibrium behavior. we establish general conditions for agreement, herding, and information aggregation in equilibrium, highlighting a connection between agreement and information aggregation.\n",
      "\n",
      "social learning equilibria. we consider a large class of social learning models in which a group of agents face uncertainty regarding a state of the world, share the same utility function, observe private signals, and interact in a general dynamic setting. we introduce social learning equilibria, a static equilibrium concept that abstracts away from the details of the given extensive form, but nevertheless captures the corresponding asymptotic equilibrium behavior. we establish general conditions for agreement, herding, and information aggregation in equilibrium, highlighting a connection between agreement and information aggregation.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## merge abstracts and titles into one big text blob without punctuation to search for keywords in\n",
    "abstract_title_concats = [title+'. '+abstract for title,abstract in zip(titles,abstracts)]\n",
    "print(titles[2] + '\\n')\n",
    "print(abstracts[2] + '\\n')\n",
    "print(abstract_title_concats[2] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we look at the keyword lists where we need at least one hit from each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['bandit', 'partial information'], ['regret', 'generalization error']]\n"
     ]
    }
   ],
   "source": [
    "print(kwd_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of a hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thompson sampling algorithms for cascading bandits. motivated by efficient optimization for online recommender systems, we revisit the cascading bandit model proposed by kveton et al. (2015). while thompson sampling (ts) algorithms have been shown to be empirically superior to upper confidence bound (ucb) algorithms for cascading bandits, theoretical guarantees are only known for the latter, not the former. in this paper, we close the gap by designing and analyzing a ts algorithm, ts-cascade, that achieves the state-of-the-art regret bound for cascading bandits. next, we derive a nearly matching regret lower bound, with information-theoretic techniques and judiciously constructed cascading bandit instances. in complement, we also provide a problem-dependent upper bound on the regret of the thompson sampling algorithm with beta-bernoulli update; this upper bound is tighter than a recent derivation by huyuk and tekin (2019). finally, we consider a linear generalization of the cascading bandit model, which allows efficient learning in large cascading bandit problem instances. we introduce a ts algorithm, which enjoys a regret bound that depends on the dimension of the linear model but not the number of items. our paper establishes the first theoretical guarantees on ts algorithms for stochastic combinatorial bandit problem model with partial feedback. numerical experiments demonstrate the superiority of our ts algorithms compared to existing ucb algorithms.\n"
     ]
    }
   ],
   "source": [
    "print(abstract_title_concats[66])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(kwd_one[0][0] in abstract_title_concats[66]) # check for bandit\n",
    "print(kwd_one[0][1] in abstract_title_concats[66]) # check for partial information\n",
    "print('')\n",
    "print(kwd_one[1][0] in abstract_title_concats[66]) # check for regret\n",
    "print(kwd_one[1][1] in abstract_title_concats[66]) # check for generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## only abstracts and titles that have intersection with required categories\n",
    "\n",
    "# for each kwd_one list take indexes such that title/abstract intersect with the list\n",
    "kwd_one_idxs_lists = [set([idx for idx,val in enumerate(list(map(lambda x: any([kwd in x for kwd in kwd_one_list]), abstract_title_concats))) if val]) for kwd_one_list in kwd_one]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{66, 86, 274, 398, 440, 484, 666, 693, 718, 759, 846, 918, 941, 971}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwd_one_idxs_lists[0] # everywhere that had bandits or partial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{66,\n",
       " 76,\n",
       " 86,\n",
       " 149,\n",
       " 214,\n",
       " 239,\n",
       " 274,\n",
       " 294,\n",
       " 326,\n",
       " 344,\n",
       " 387,\n",
       " 398,\n",
       " 440,\n",
       " 462,\n",
       " 503,\n",
       " 549,\n",
       " 550,\n",
       " 666,\n",
       " 693,\n",
       " 728,\n",
       " 741,\n",
       " 759,\n",
       " 787,\n",
       " 846,\n",
       " 941,\n",
       " 949,\n",
       " 971}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwd_one_idxs_lists[1] # everywhere that had regret or generalization error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{66, 86, 274, 398, 440, 666, 693, 759, 846, 941, 971}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take intersection of all the kwd_one index sets\n",
    "kwd_one_idxs = kwd_one_idxs_lists[0].intersection(*kwd_one_idxs_lists) \n",
    "kwd_one_idxs # everywhere that has at least one from each list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{66, 398, 759}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if len(kwd_req) > 0:\n",
    "    kwd_req_idxs = set([idx for idx,val in enumerate(list(map(lambda x: all([kwd in x for kwd in kwd_req]), abstract_title_concats))) if val])\n",
    "else:\n",
    "    kwd_req_idxs = set(range(len(abstract_title_concats)))\n",
    "if len(kwd_exc) > 0:\n",
    "    kwd_exc_idxs = set([idx for idx,val in enumerate(list(map(lambda x: all([kwd not in x for kwd in kwd_exc]), abstract_title_concats))) if val])\n",
    "else:\n",
    "    kwd_exc_idxs = set(range(len(abstract_title_concats)))\n",
    "    \n",
    "kwd_idxs = kwd_one_idxs.intersection(kwd_req_idxs, kwd_exc_idxs)\n",
    "kwd_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do any author restrictions here, but the code works exactly the same as for keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we print a dataframe with the papers that we wanted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>categories</th>\n",
       "      <th>url</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>thompson sampling algorithms for cascading ban...</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>1810.01187</td>\n",
       "      <td>motivated by efficient optimization for online...</td>\n",
       "      <td>[cs.lg, stat.ml]</td>\n",
       "      <td>https://arxiv.org/abs/1810.01187</td>\n",
       "      <td>[wang chi cheung, zixin zhong, vincent y. f. tan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>optimal no-regret learning in repeated first-p...</td>\n",
       "      <td>2020-05-08</td>\n",
       "      <td>2003.09795</td>\n",
       "      <td>we study online learning in repeated first-pri...</td>\n",
       "      <td>[cs.lg, cs.gt, cs.it, math.it, stat.me, stat.ml]</td>\n",
       "      <td>https://arxiv.org/abs/2003.09795</td>\n",
       "      <td>[yanjun han, zhengyuan zhou, tsachy weissman]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>dtr bandit: learning to make response-adaptive...</td>\n",
       "      <td>2020-05-06</td>\n",
       "      <td>2005.02791</td>\n",
       "      <td>dynamic treatment regimes (dtrs) for are perso...</td>\n",
       "      <td>[stat.ml, cs.lg, math.oc]</td>\n",
       "      <td>https://arxiv.org/abs/2005.02791</td>\n",
       "      <td>[yichun hu, nathan kallus]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title        date  \\\n",
       "66   thompson sampling algorithms for cascading ban...  2020-05-08   \n",
       "398  optimal no-regret learning in repeated first-p...  2020-05-08   \n",
       "759  dtr bandit: learning to make response-adaptive...  2020-05-06   \n",
       "\n",
       "             id                                           abstract  \\\n",
       "66   1810.01187  motivated by efficient optimization for online...   \n",
       "398  2003.09795  we study online learning in repeated first-pri...   \n",
       "759  2005.02791  dynamic treatment regimes (dtrs) for are perso...   \n",
       "\n",
       "                                           categories  \\\n",
       "66                                   [cs.lg, stat.ml]   \n",
       "398  [cs.lg, cs.gt, cs.it, math.it, stat.me, stat.ml]   \n",
       "759                         [stat.ml, cs.lg, math.oc]   \n",
       "\n",
       "                                  url  \\\n",
       "66   https://arxiv.org/abs/1810.01187   \n",
       "398  https://arxiv.org/abs/2003.09795   \n",
       "759  https://arxiv.org/abs/2005.02791   \n",
       "\n",
       "                                               authors  \n",
       "66   [wang chi cheung, zixin zhong, vincent y. f. tan]  \n",
       "398      [yanjun han, zhengyuan zhou, tsachy weissman]  \n",
       "759                         [yichun hu, nathan kallus]  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## only take temp_df rows that match the desired indices for keywords and authors\n",
    "records_df = records_df.iloc[list(kwd_idxs)]\n",
    "records_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's export this table to a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/blairbilodeau/Desktop/arxiv/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2020-05-22-arxiv-metadata.csv'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(export + '\\n')\n",
    "exportfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-22-arxiv-metadata.csv\n",
      "\n",
      "/Users/blairbilodeau/Desktop/arxiv/2020-05-22-arxiv-metadata.csv\n"
     ]
    }
   ],
   "source": [
    "# file to export csv to\n",
    "if exportfile == '':\n",
    "    exportfile = datetime.date.today().strftime('%Y-%m-%d') + '-arxiv-metadata.csv'\n",
    "exportpath = export + exportfile\n",
    "\n",
    "print(exportfile + '\\n')\n",
    "print(exportpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "records_df.to_csv(exportpath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also export the pdfs themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/blairbilodeau/Desktop/arxiv/'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folder to download pdfs to\n",
    "download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://arxiv.org/pdf/1810.01187.pdf', 'https://arxiv.org/pdf/2003.09795.pdf', 'https://arxiv.org/pdf/2005.02791.pdf']\n"
     ]
    }
   ],
   "source": [
    "## create urls to access pdfs\n",
    "pdf_urls = ['https://arxiv.org/pdf/' + id + '.pdf' for id in records_df.id] # list of urls to pull pdfs from\n",
    "\n",
    "print(pdf_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/blairbilodeau/Desktop/arxiv/2020-05-08_cheung_zhong_tan.pdf', '/Users/blairbilodeau/Desktop/arxiv/2020-05-08_han_zhou_weissman.pdf', '/Users/blairbilodeau/Desktop/arxiv/2020-05-06_hu_kallus.pdf']\n"
     ]
    }
   ],
   "source": [
    "# create filenames to export pdfs to\n",
    "# currently setup in year_lastname format\n",
    "pdf_lastnames_full = ['_'.join([name.split()[-1] for name in namelist]) for namelist in records_df.authors] # pull out lastnames only\n",
    "pdf_lastnames = [name if len(name) < 200 else name.split('_')[0] + '_et_al' for name in pdf_lastnames_full] # make sure file names don't get longer than ~200 chars\n",
    "pdf_paths = [download + date + '_' + lastname + '.pdf' for date,lastname in zip(records_df.date, pdf_lastnames)] # full path for each file\n",
    "\n",
    "print(pdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete.\n"
     ]
    }
   ],
   "source": [
    "# export pdfs\n",
    "for paper_idx in range(len(pdf_urls)):\n",
    "    response = requests.get(pdf_urls[paper_idx])\n",
    "    file = open(pdf_paths[paper_idx], 'wb')\n",
    "    file.write(response.content)\n",
    "    file.close()\n",
    "    gc.collect()\n",
    "print('Download complete.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
